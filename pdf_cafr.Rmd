---
title: "R Notebook"
output: html_notebook
---

```{r}
library(pdftools)
library(tabulizer)
library(stringr)
library(data.table)
library(rlist)
library(pipeR)
#library(readxl)
```

```{r}
r <- readRDS("mass.RDS")
specs <- r$specs
pdf_list <- r$pdf_list
table <- r$table

rm(r)
```


```{r 'get-cities'}

# Get city names from Reason spreadsheet
cities <- 
  readxl::read_excel(
    "/Users/davidlucey/Desktop/David/Projects/mass_munis/reason.xlsx",
    sheet = "GSheets Long",
    range = "B1:B151",
    col_names = TRUE)
names(cities) <- cities[1,]
cities <- cities$Entity[2:150]

# Prepare cities for url paste
cities_url <- 
  ifelse(
    str_detect(cities, "\\w\\s\\w"),
    paste0(str_extract(cities, "^\\w+"),
           "%20",
           str_extract(cities, "\\w+$")),
    cities)

# Create list of pdfs
dir <- "/Users/davidlucey/Desktop/David/Projects/mass_munis/data/pdf_cafr/"
files <- list.files(dir)
city <- str_remove(files, "_2018.pdf")
pdfs <- 
  paste0(dir, city, "_2018.pdf")

```


```{r 'get-pdfs', eval=FALSE, include=FALSE}

# Apply download.file to url to download specified pdfs and save in data/pdf_cafr folder
year <- "2018"
lapply(cities_url[c(1,10)], function(city) {
  try(download.file(
    paste0("https://cafr.file.core.windows.net/cafr/General%20Purpose/2018/MA%20",city,"%202018.pdf?sv=2017-07-29&ss=f&srt=sco&sp=r&se=2120-03-06T21:34:56Z&st=2020-03-06T21:34:56Z&spr=https&sig=lFqyP8wwH1giyaFhjj6lCVuaAZ9xgbSnjEtzEtpusgA%3D",collapse=""),
    destfile = 
      paste0(
        "data/pdf_cafr/",
        tolower(str_replace(city, "\\%20", "_")),
        "_", year,
        ".pdf"),
    mode = "wb"))})

```


```{r 'select-pdfdata'}

pdf_list <- 
  
  lapply(pdfs, try(function(pdf){

    # Keep only 1st 75 pages
    rpt <- pdf_data(pdf)[1:75]
    
    # Name by index for each muni
    names(rpt) <- 1:length(rpt)
    
    rpt <- 
      rpt[unlist(lapply(rpt, function(page) {
        # Filter pages with notes to financial statement language
        ((str_detect(
          tolower(paste(page$text[page$y %in% tail(unique(page$y), 5)], collapse = " ")), 
          "notes to basic financial statements|accompanying notes|integral part of these financial statements|notes to the financial statements are an integral part of this statement"
        ) |
        # Filter pages with key statemnt names
           str_detect(
          tolower(paste(page$text[page$y %in% head(unique(page$y), 5)], collapse = " ")), 
          "statement of net position|statement of activities|statement of revenues|balance sheet"
          )
          ) & 
        !str_detect(
        # Then drop pages with these phrases
          tolower(paste(page$text, collapse = " ")), 
          "discussion & analysis|contents|discussion and analysis|fiduciary|enterprise|proprietary|reconciliation|combining|comparative|highlights|budget|non(-)major"
        ))
      }))]
    
    # Convert to dt
    rpt <- lapply(rpt, setDT)
    
    names <- as.integer(names(rpt))
    mean_names <- mean(names)
    rpt <- rpt[names - mean_names < 10]
    
    # Return 
    rpt
    
  }))

# Name pdf_list by muni
names(pdf_list) <- tolower(cities)


```


NOT USED
```{r eval=FALSE, include=FALSE}
np <- pdf_list %>%
      list.map(x ~ x[unlist(lapply(x, try(function(page) {
    text <- paste(page$text[page$y < 200], collapse = " ")
    any(str_detect(tolower(text), "statement of activities"))
    })))]) 
```

NOT USED
```{r eval=FALSE, include=FALSE}
  
test <-
  pdf_list %>>%
  # Keep tables with "june" and "2018" at top
    list.map(x ~ x[unlist(lapply(x, try(function(page) {
    text <- paste(page$text[page$y < 200], collapse = " ")
    any(str_detect(tolower(text), "june|2018|continued"))
    })))]) 

test <- pdf_list%>>%
   # Keep only chart pages based on digit/letter ratio > 0.2
    list.map(x ~ x[lapply(x, function(page) {
    text <- paste(page$text, collapse = " ")
    d <- str_count(text, "\\d")
    w <- str_count(text, "[[:alpha:]]")
    d / w
  }) > 0.10]) 

pdf_list <-
  pdf_list %>>%
  # Keep rows with "governmental", "primary" or "statement" at top
  list.map(x ~ x[unlist(lapply(x, try(function(page) {
    
    # Find top row & filter page$text
    top_y <- 
      min(c(page$y[str_detect(page$text, "\\,\\d{3}")], 200))
    date_y <- 
      min(c(page$y[str_detect(tolower(page$text), "june")], top_y))
    top_y <- 
      ifelse(date_y %between% c(1, 200),
             date_y,
             ifelse(top_y %between% c(1, 200), top_y, 200))
    top <-
      ifelse(date_y < top_y, date_y, top_y)
    text <- 
      tolower(page$text[page$y < top])
    
    # Pages to keep
    keeps <- 
      c(
        "(?<!.)governmental(?!.)",
        "statement",
        "primary",
        "continued"
        )
    
    # Regex and filter list
    keep_pattern <- paste(keeps, collapse = "|")
    any(str_detect(text, keep_pattern))
    
  })))]) 

pdf_list <- 
  pdf_list %>%
    # Drop non-governmental
    list.map(x ~ x[unlist(lapply(x, try(function(page) {
    
    # Find top row & filter page$text
    top_y <- 
      min(page$y[str_detect(page$text, "\\,\\d{3}")])
    date_y <- 
      min(c(page$y[str_detect(tolower(page$text), "june")], top_y))
    top_y <- 
      ifelse(date_y %between% c(1, 200),
             date_y,
             ifelse(top_y %between% c(1, 200), top_y, 200))
    top <-
      ifelse(date_y < top_y, date_y, top_y)
    text <-
      tolower(page$text[page$y < top])
  
    # Pages to drop
    drops <- 
      c(
        "proprietary",
        "fiduciary",
        "supplementary",
        "tax(es)?",
        "pension(s)?",
        "opeb",
        "benefit",
        "budget(ary)?",
        "schedule(s)?",
        "non-major",
        "nonmajor",
        "auditors",
        "agency",
        "ten",
        "internal",
        "business",
        "notes",
        "combining",
        "comparative",
        "highlights",
        "condensed",
        "management's",
        "liabilities",
        "debt"
    )
    
    # Regex and filter list 
    drop_pattern <- paste(drops, collapse = "|")
    ! any(str_detect(text, drop_pattern))
    
    })))])
  
```


```{r 'calc-table-area'}
calc_table_area <- function(dt) {
  
  #dt <- pdf_list[["attleboro"]][["22"]]
  
  # Get table params
  # Note this function is applied after filtering header
  
  # Top  
  top <- min(dt$y) - 25
  
  # Left
  left <- min(dt$x) - 25
  
  # Right
  right <- max(dt$x) + 45
  
  # Bottom
  bottom <- max(dt$y) + 25 
  
  # Area list
  a <- c(top, left, bottom, right)
  
  # Return
  a
}
```



```{r 'get-table-specs'}

specs <- 
  
  pdf_list[unlist(lapply(pdf_list, length))>0] %>>%
  
  list.map(x ~ lapply(x, function(page) {
    
    # Convert to dt
    page <- setDT(page)

    # Horizontal
    x <- 8.5 * 72
    y <- 11 * 72
    max_x <- max(page$x)
    max_y <- max(page$y)
    orientation <- 
      ifelse(x < max_x, "horizontal", "verticle")
    
    # Top
    table_top <-
      min(page$y[str_detect(page$text, "2018") & page$space==FALSE])
    height_top <- unique(page$height[page$y == table_top])
    top <- table_top + height_top 
    
    table_bottom <-
       max(page$y[str_detect(page$text, "\\$")])
    height_bottom <- unique(page$height[page$y == table_bottom])
    bottom <- table_bottom + height_bottom
  
    # Convert empty pages to null
    if(table_top == max_y | table_bottom == max_x) { page <- NULL }
    
       # Left
    left <-     
      ifelse( min(page$x) - 30 > 0,
              min(page$x) - 30, 1 )
    
    # Right
    width_max_x <- max(page$width[page$x == max_x])
    right <- 
    max_x + width_max_x + ifelse(orientation == "verticle", 30, 50)
    
    
    # Area list
    a <- c(top, left, bottom, right)
  
    # Return
    a
    
    })) %>>% 
  
  # Drop null pages
  list.map(x ~ x[!sapply(x, function(x) is.null(x))]) 

#%>>%
  # Apply calc_table_area to each page subset
  #list.map(x ~ lapply(x, function(page) {
  #  calc_table_area(page)
  # }))

```

NOT USED
```{r}
    
test  <- 
  pdf_list %>>%
  list.map(x ~ lapply(x, function(page) {
    page <- setDT(page)
    y_drops <- 
      page$y[str_detect(tolower(page$text[page$y < 200]), "june|town|notes|comprehensive|primary|governmental|program\\srevenues")]
    page <- 
      page[!page$y %in% y_drops]
    line_drops <- c("\\.*", "\\–", "	-", "\\$", "	-","")
    line_pattern <- 
      paste("(?<!.)", line_drops, "(?!.)", collapse="|", sep="")
    page <- 
      page[!str_detect(page$text, line_pattern)]
    page <- page[!is.null(page$text)]
    # Remove punctuatuation from remaining pages in text list
    #page[, text := tm::removePunctuation(text)]
    page[, text := str_remove(text, "\\W\\……*(?!.)")]
    page
    }))

```




```{r 'get-table'}

table  <- 
  
  mapply(function(x, y) {
    
    # Debug - remove later
    #x <- specs[[7]][2]
    #y <- "attleboro"
    # a <- specs[["attleboro"]][["22"]]
    
    # Params from mapply
    a <- x
    page <- as.integer(names(x))
    city <- gsub(" ", "_", y)
    
    print(city)
     
    # Set up pdf using city
    dir <- 
      "/Users/davidlucey/Desktop/David/Projects/mass_munis/data/pdf_cafr/"
    pdf <-
      paste0(dir, city, "_2018.pdf")
    
    # Loop to apply params in tabulizer
    l <- mapply(function(a, page, pdf) {
      
      print(page)
      
      # Tabulizer
      if(length(a) == 4) {
        t <- 
          try(extract_tables(pdf, 
                      pages = page, 
                      area = list(a), 
                      guess = F,
                      output = "data.frame"))
      } else { t <- data.frame() }
      
      }, a, page, pdf)
    
    # Loop to clean up and drop unneeded cols
    l <- lapply(l, function(t) {
      
      # Convert to dt
      t <- setDT(t)
      
      # Select $ or all is.na columns and drop
      if(length(t) > 0) {
        drops <-
          sapply(t, function(col) which(any(str_detect(col, "^\\$$")) | all(is.na(col))))
        drops <- 
          which(sapply(drops, function(col) sum(col) > 0))
        t[ , (drops) := NULL] 
        }
      
      # Return dt
      t
      
    })
    
    l
  
}, specs, names(specs))

```





```{r 'calc-header-area', eval=FALSE, include=FALSE}

# Function to extract header and first few rows of page
# First few rows are extracted to guide columns

calc_header_area <- function(dt) {
  
  # Top  
  top <- min(dt$y)
  height <- unique(dt$height[dt$y == top])
  top <- top - height - 1 
  
  # Left
  left <- 
    ifelse( min(dt$x) - 30 > 0,
            min(dt$x) - 30, 1 )
  
  # Right
  width_max_x <- dt$width[dt$x == max(dt$x)]
  max_x <- max(dt$x)
  right <- max_x + width_max_x + 25
  
  # Bottom
  bottom <- max(dt$y) + 10
  
  # Area list
  a <- c(top, left, bottom, right)
  
  # Return
  a
  
}
```


```{r eval=FALSE, include=FALSE}
calc_header_area <- function(dt) {
  
  # Top  
  top <- max(dt$y)
  
  # Left
  left <- 
    ifelse( min(dt$x) - 30 > 0,
            min(dt$x) - 30, 1 )
  
  # Right
  width_max_x <- unique(dt$width[dt$x == max(dt$x)])
  max_x <- max(dt$x)
  right <- max(max_x + width_max_x + 25)
  
  # Bottom
  bottom <- min(setdiff(unique(dt$y), min(dt$y)))
  
  
  # Area list
  a <- c(top, left, bottom, right)
  
  # Return
  a
  
}
```


```{r 'get-header-specs', eval=FALSE, include=FALSE}

header <- 
  
  pdf_list[unlist(lapply(pdf_list, length))>0] %>>%
  
  list.map(x ~ lapply(x, function(page) {
    
    #page <- pdf_list[[1]]$`16`
    
    # Convert data.table
    page <- setDT(page)
    
    # Get max y row
    y_max <- max(page$y)
    #height_y_max <- unique(page$height[page$y == y_max])
    x_max <- max(page$x)
    
    
    # Bottom
    header_bottom <- 
      min(c(page$y[str_detect(page$text, "\\$") & page$y < 250], y_max),na.rm=TRUE)
    header_bottom <- 
      min(setdiff(unique(page$y[page$y > header_bottom]), header_bottom))
    
    header_top <- 
      min(c(page$y[str_detect(page$text, "(?<!.)2018(?!.)") & page$space == FALSE], y_max), na.rm = TRUE)
    
    # Convert empty pages to null
    if(header_bottom == y_max | header_top == y_max) { 
      page <- data.table() 
      } else { 
      page <- 
        page[page$y <= header_bottom &
               page$y > header_top]
      }
    
    # Return
    #page 
    
    # Top  
  top <- min(page$y)
  height <- unique(page$height[page$y == top])
  top <- top - height - 1 
  
  # Left
  left <- 
    ifelse( min(page$x) - 30 > 0,
            min(page$x) - 30, 1 )
  
  # Right
  width_max_x <- page$width[page$x == max(page$x)]
  max_x <- max(page$x)
  right <- max_x + width_max_x + 25
  
  # Bottom
  bottom <- max(page$y) + 10
  
  # Area list
  a <- c(top, left, bottom, right)
  
  # Return
  a
    
    })) %>>% 
  
  # Drop null pages
  list.map(x ~ x[!sapply(x, function(x) nrow(x) == 0)]) %>>%
  
  # Apply calc_table_area to each page 
  list.map(x ~ lapply(x, function(page) {
    calc_header_area(page)
   }))

```



```{r eval=FALSE, include=FALSE}

header <- 
  
  pdf_list[unlist(lapply(pdf_list, length))>0] %>>%
  
  list.map(x ~ lapply(x, function(page) {
    
    #page <- pdf_list[[1]]$`16`
    #page <- pdf_list[[4]]$`43`
    
    # Convert data.table
    page <- setDT(page)
    
    # Horizontal
    x <- 8.5 * 72
    y <- 11 * 72
    max_x <- max(page$x)
    max_y <- max(page$y)
    orientation <- 
      ifelse(x < max_x, "horizontal", "verticle")
    
    # Top
    top_y <- 
      min(unique(page$y[str_detect(page$text, "(?<!.)2018(?!.)") & 
                     page$space == FALSE]))
    top_height <- unique(page$height[page$y == top_y])
    top <- top_y + top_height
      
    # Left
    left <-     
      ifelse( min(page$x) - 30 > 0,
              min(page$x) - 30, 1 )
    
    # Bottom
    table_top <- 
      unique(page$y[str_detect(page$text, "\\$") & page$y < 250])
    bottom_one <- 
      min(unique(page$y[page$y > table_top]), max_y, na.rm=TRUE)
    bottom <- 
      min(unique(page$y[page$y > bottom_one]), max_y, na.rm=TRUE)
      
    # Right
    width_max_x <- max(page$width[page$x == max_x])
    right <- 
      max_x + width_max_x + ifelse(orientation == "verticle", 30, 50)
    
    # Area list
    a <- c(top, left, bottom, right)
  
    # Return
    a 
  
  })) %>>% 
  
  # Drop null pages
  list.map(x ~ x[!sapply(x, function(x) any(is.infinite(x)))])
  
```


try to get header out of text
```{r eval=FALSE, include=FALSE}
https://blog.az.sg/posts/reading-pdfs-in-r/

page[, fcoalesce(text), by="y"]
fcoalesce(page$text, sep="")[page$y == 115]
tabulize(page)
test <- extract_text(pdfs[3], pages=16) %>%
  readr::read_lines()
test <- test[1:min(which(str_detect(test, "\\$"))) -1]
writeLines(test)

result <- pdf_text(pdfs[3])[16] %>%
  # split filecontent by newline character
  str_split("\n") %>%
  # convert to tibble and assign unique column names
  as_tibble(.name_repair = make.names) 

result <- pdf_text(pdfs[3])[16] %>%
 str_split("\n") %>%
  first() %>%
  as_tibble()
result <- result[1:min(which(str_detect(result$value, "\\$"))) -1,]
```


tabulize function for pdf_data
```{r eval=FALSE, include=FALSE}
#https://stackoverflow.com/questions/60127375/using-the-pdf-data-function-from-the-pdftools-package-efficiently
tabulize <- function(pdf_df, filter = 0.01)
{
  pdf_df <- page
  xd <- density(pdf_df$x, filter)
  yd <- density(pdf_df$y, filter)
  pdf_df$col <- as.numeric(cut(pdf_df$x, c(xd$x[xd$y > .5] - 2, max(xd$x) + 3)))
  pdf_df$row <- as.numeric(cut(pdf_df$y, c(yd$x[yd$y > .5] - 2, max(yd$x) + 3)))
  pdf_df %<>% group_by(row, col) %>% summarise(label = paste(text, collapse = " "))
  res <- matrix(rep("", max(pdf_df$col) * max(pdf_df$row)), nrow = max(pdf_df$row))
  for(i in 1:nrow(pdf_df)) res[pdf_df$row[i], pdf_df$col[i]] <- pdf_df$label[i]
  res <- res[which(apply(r, 1, paste, collapse = "") != ""), ]
  res <- res[,which(apply(r, 2, paste, collapse = "") != "")]
  as.data.frame(res[-1,])
}
```


Header table NOT USED
```{r 'get-header-table', eval=FALSE, include=FALSE}

header_table  <- 
  
  mapply(function(x, y) {
    
    #x <-  header[["braintree"]][["39"]]
    #y <- "braintree"
    #page <- 39
    #page <- pdf_list[["amherst"]][["16"]]

    # Params
    a <- x
    page <- as.integer(names(x))
    city <- gsub(" ", "_", y)
     
    # Set up pdf
    pdf <-
      paste0(dir, city, "_2018.pdf")
    
    # Extract table with params and pdf with mapply
    l <- mapply(function(a, page, pdf) {
      
      # Tabulizer
      if(length(a) == 4) {
        t <- 
          try(extract_tables(pdf, 
                      pages = page, 
                      area = list(a), 
                      guess = F,
                      output = "data.frame"))
      } else { t <- data.frame() }
  
      t
      
      }, a, page, pdf)
    
    # Convert to dt and drop unneeded cols
    l <- lapply(l, function(t) {
      
      # Convert to dt
      t <- setDT(t)
      
      # Select $ or all is.na columns and drop
      if(length(t) > 0) {
        drops <-
          sapply(t, function(col) which(any(str_detect(col, "^\\$$")) | all(is.na(col))))
        drops <- 
          which(sapply(drops, function(col) sum(col) > 0))
        t[ , (drops) := NULL] 
        }
      
      # Return dt
      t
      
    })
    
    # Return list for city
    l
  
}, header, names(header))

# Add subscript "a" to header_table municipal index names
header_table <- 
  sapply(header_table, function(list) {
    names(list) <- paste(names(list), "a",sep="")
    list})

```


Merger of header and table NOT USED
```{r 'get-spreadsheets', eval=FALSE, include=FALSE}

n <- mapply(c, header_table, table)

n1 <- 
  lapply(n, function(x) {
    doubles <-
      names(which(table(str_remove(names(x), "a")) == 2))
  l <- 
    lapply(doubles, try(function(double) {
      subset <-
        x[str_detect(names(x), double)]
      new <-
        try(rbindlist(subset, use.names = FALSE)
        )
    }))
  names(l) <- doubles
  l
  })

```


```{r}
saveRDS(list(specs=specs,
             table=table,
             pdf_list=pdf_list), 
        "mass.RDS")
```



```{r 'save-xlsx'}
library(xlsx)
# Save to workbook with each list element as tab
wb <- createWorkbook()
example <- n1[[1]][1:2]
example <- c(example,n1[[2]][1:2])
sheetnames <- names(example)
sheets <- lapply(sheetnames, createSheet, wb = wb)
void <- Map(addDataFrame, example, sheets)
saveWorkbook(wb, file = "sample_cafr.xlsx")
```




